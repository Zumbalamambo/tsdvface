{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Helper for evaluation on the Asian Celeb dataset\"\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import facenet\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n",
    "    # Calculate evaluation metrics\n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    embeddings1 = embeddings[0::2]\n",
    "    embeddings2 = embeddings[1::2]\n",
    "    \n",
    "    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\n",
    "                      np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    \n",
    "    thresholds = np.arange(0, 4, 0.01)\n",
    "    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2, np.asarray(actual_issame),\n",
    "                     1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    \n",
    "    return tpr, fpr, accuracy, val, val_std, far\n",
    "\n",
    "def get_paths(acd_dir, pairs):\n",
    "    nrof_skipped_pairs = 0\n",
    "    path_list = []\n",
    "    issame_list = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        if len(pair) == 3:    # make all matches\n",
    "            path0 = add_extension(os.path.join(acd_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(acd_dir, pair[0], pair[2]))\n",
    "            issame = True\n",
    "        elif len(pair) == 4:    # make all mismatches\n",
    "            path0 = add_extension(os.path.join(acd_dir, pair[0], pair[1]))\n",
    "            path1 = add_extension(os.path.join(acd_dir, pair[2], pair[3]))\n",
    "            issame = False\n",
    "        if os.path.exists(path0) and os.path.exists(path1):   # Only add the pair if both paths exist\n",
    "            path_list += (path0, path1)\n",
    "            issame_list.append(issame)\n",
    "        else:\n",
    "            nrof_skipped_pairs += 1\n",
    "    if nrof_skipped_pairs > 0:\n",
    "        print('Skipped %d image pairs' % nrof_skipped_pairs)\n",
    "        \n",
    "    return path_list, issame_list\n",
    "\n",
    "def add_extension(path):\n",
    "    if os.path.exists(path + '.jpg'):\n",
    "        return path + '.jpg'\n",
    "    elif os.path.exists(path + '.png'):\n",
    "        return path + '.png'\n",
    "    else:\n",
    "        raise RuntimeError('No file \"%s\" with extension png or jpg.' % path)\n",
    "        \n",
    "def read_pairs(pairs_filename):\n",
    "    pairs = []\n",
    "    with open(pairs_filename, 'r') as f:\n",
    "        for line in f.readlines()[0:]:\n",
    "            pair = line.strip().split()\n",
    "            pairs.append(pair)\n",
    "    return np.array(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate on Asian Celeb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import facenet\n",
    "import lfw\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acd(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n",
    "        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n",
    "    # Run forward pass to calculate embeddings\n",
    "    print('Runnning forward pass on Asian Celeb Dataset (ACD) images')\n",
    "    \n",
    "    # Enqueue one epoch of image paths and labels\n",
    "    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n",
    "    nrof_flips = 2 if use_flipped_images else 1\n",
    "    nrof_images = nrof_embeddings * nrof_flips\n",
    "    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n",
    "    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n",
    "    control_array = np.zeros_like(labels_array, np.int32)\n",
    "    if use_fixed_image_standardization:\n",
    "        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n",
    "    if use_flipped_images:\n",
    "        # Flip every second image\n",
    "        control_array += (labels_array % 2)*facenet.FLIP\n",
    "    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n",
    "    \n",
    "    embedding_size = int(embeddings.get_shape()[1])\n",
    "    print (nrof_embeddings, nrof_flips)\n",
    "    print (nrof_images, batch_size)\n",
    "    assert nrof_images % batch_size == 0, 'The number of ACD images must be an integer multiple of the ACD batch size'\n",
    "    nrof_batches = nrof_images // batch_size\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "    lab_array = np.zeros((nrof_images,))\n",
    "    for i in range(nrof_batches):\n",
    "        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n",
    "        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n",
    "        lab_array[lab] = lab\n",
    "        emb_array[lab, :] = emb\n",
    "        if i % 10 == 9:\n",
    "#             print('.', end='')\n",
    "            sys.stdout.flush()\n",
    "    print('')\n",
    "    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n",
    "    if use_flipped_images:\n",
    "        # Concatenate embeddings for flipped and non flipped version of the images\n",
    "        embeddings[:,:embedding_size] = emb_array[0::2,:]\n",
    "        embeddings[:,embedding_size:] = emb_array[1::2,:]\n",
    "    else:\n",
    "        embeddings = emb_array\n",
    "\n",
    "    assert np.array_equal(lab_array, np.arange(nrof_images))==True, 'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline'\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n",
    "    \n",
    "    print('Accuracy: %2.5f+-%2.5f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "    \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "    #eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    #print('Equal Error Rate (EER): %1.3f' % eer)\n",
    "    \n",
    "ACD_PAIRS = './pairs.txt'    # The file containing the pairs to use for validation.\n",
    "ACD_DIR = '/home/hungnv/master/src/facenet/data/celebrity_1000/' # Path to the data directory containing aligned LFW face patches.\n",
    "IMAGE_SIZE = 160    # Image size (height, width) in pixels.\n",
    "ACD_BATCH_SIZE = 100    # Number of images to process in a batch in the ACD test set.\n",
    "ACD_NROF_FOLDS = 10    # Number of folds to use for cross validation. Mainly used for testing.\n",
    "DISTANCE_METRIC = 0    # Distance metric  0:euclidian, 1:cosine similarity.\n",
    "SUBTRACT_MEAN = 'store_true'    # Subtract feature mean before calculating distance.\n",
    "USE_FLIPPED_IMAGES = 'store_true'    # Concatenates embeddings for the image and its horizontally flipped counterpart.\n",
    "USE_FIXED_STD = 'store_true'\n",
    "MODEL_DIR = '/home/hungnv/master/src/Face_Recognition-master/lib/src/ckpt/20180402-114759/'\n",
    "def main():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Read the file containing the pairs used for testing\n",
    "            pairs = read_pairs(os.path.expanduser(ACD_PAIRS))\n",
    "            \n",
    "            # Get the paths for the corresponding images\n",
    "            paths, actual_issame = get_paths(os.path.expanduser(ACD_DIR), pairs)\n",
    "            \n",
    "            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name='image_paths')\n",
    "            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name='labels')\n",
    "            batch_size_placeholder = tf.placeholder(tf.int32, name='batch_size')\n",
    "            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name='control')\n",
    "            phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n",
    "            \n",
    "            nrof_preprocess_threads = 4\n",
    "            image_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
    "            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n",
    "                                        dtypes=[tf.string, tf.int32, tf.int32],\n",
    "                                        shapes=[(1,), (1,), (1,)],\n",
    "                                        shared_name=None, name=None)\n",
    "            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name='eval_enqueue_op')\n",
    "            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n",
    "            \n",
    "            # Load the model\n",
    "            input_map = {'image_batch': image_batch, 'label_batch': label_batch, 'phase_train': phase_train_placeholder}\n",
    "            facenet.load_model(MODEL_DIR, input_map=input_map)\n",
    "            \n",
    "            # Get output tensor\n",
    "            embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "            \n",
    "            coord = tf.train.Coordinator()\n",
    "            tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "            evaluate_acd(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n",
    "                embeddings, label_batch, paths, actual_issame, ACD_BATCH_SIZE, ACD_NROF_FOLDS, DISTANCE_METRIC, SUBTRACT_MEAN,\n",
    "                USE_FLIPPED_IMAGES, USE_FIXED_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/hungnv/master/src/Face_Recognition-master/lib/src/ckpt/20180402-114759/\n",
      "Metagraph file: model-20180402-114759.meta\n",
      "Checkpoint file: model-20180402-114759.ckpt-275\n",
      "INFO:tensorflow:Restoring parameters from /home/hungnv/master/src/Face_Recognition-master/lib/src/ckpt/20180402-114759/model-20180402-114759.ckpt-275\n",
      "Runnning forward pass on Asian Celeb Dataset (ACD) images\n",
      "(1200, 2)\n",
      "(2400, 100)\n",
      "\n",
      "Accuracy: 0.85167+-0.05550\n",
      "Validation rate: 0.12683+-0.05575 @ FAR=0.00000\n",
      "Area Under Curve (AUC): 0.828\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input_producer, input_producer/RandomShuffle)]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
